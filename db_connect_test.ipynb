import tensorflow as tf
import pandas as pd
import pymysql
from sklearn.preprocessing import RobustScaler
import numpy as np
import time
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime

# êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì„¤ì •
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = ServiceAccountCredentials.from_json_keyfile_name("my_cred.json", scope)
client = gspread.authorize(creds)
sheet = client.open("Fire Prediction Log").sheet1  # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = tf.keras.models.load_model('fire_anomaly_model.h5')

# DB ì—°ê²°
def dbconnect():
    return pymysql.connect(
        host='localhost',
        port=3306,
        user='test1',
        password='P@ssw0rd',
        db='test',
        charset='utf8'
    )

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
def fetch_data(conn):
    query = "SELECT * FROM sensor_data"
    df = pd.read_sql(query, conn)
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df = df.sort_values('Date').reset_index(drop=True)

    features = ['CO_Room', 'Temperature_Room', 'Humidity_Room', 'PM10_Room']
    df = df.dropna(subset=features)
    df['CO_Room'] = df['CO_Room'].clip(lower=0)

    return df, features

# ì‹œí€€ìŠ¤ ìƒì„±
def create_sequences(df, features, window_size=60):
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(df[features])
    X_seq = []
    for i in range(len(X_scaled) - window_size):
        X_seq.append(X_scaled[i:i + window_size])
    return np.array(X_seq)

# ì‹œíŠ¸ì— ì˜ˆì¸¡ ë¡œê·¸ ê¸°ë¡
def log_to_sheet(index, prob, label):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    sheet.append_row([now, index, round(prob, 4), "ê²½ê³ " if label == 1 else "ì •ìƒ"])

# ì‹œíŠ¸ì— ê²½ê³  ì‹œ ì›ë³¸ ë°ì´í„° ê¸°ë¡
def log_window_data(window_df):
    sheet.append_row(["--- ìµœê·¼ 60ê°œ ì„¼ì„œê°’ (ê²½ê³  ë°œìƒ ì‹œì ) ---"])
    sheet.append_row(["Date", "CO_Room", "Temperature_Room", "Humidity_Room", "PM10_Room"])
    for _, row in window_df.iterrows():
        sheet.append_row([
            row['Date'].strftime("%Y-%m-%d %H:%M:%S"),
            row['CO_Room'], row['Temperature_Room'],
            row['Humidity_Room'], row['PM10_Room']
        ])

# ì „ì²´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
conn = dbconnect()
df, features = fetch_data(conn)
conn.close()

# ì‹œí€€ìŠ¤ ìƒì„±
X_seq = create_sequences(df, features)

print(f"ì´ ì‹œí€€ìŠ¤ ê°œìˆ˜: {len(X_seq)}\nì‹œë®¬ë ˆì´ì…˜ ì‹œì‘...")

for i in range(len(X_seq)):
    input_seq = X_seq[i].reshape(1, 60, 4)
    prob = model.predict(input_seq, verbose=0)[0][0]
    label = int(prob > 0.8)

    # ì½˜ì†” ì¶œë ¥
    print(f"[{i+1}/{len(X_seq)}] ğŸ”¥ í™”ì¬ í™•ë¥ : {prob:.4f} â†’ {'ğŸš¨ ê²½ê³ ' if label == 1 else 'âœ… ì •ìƒ'}")

    # êµ¬ê¸€ ì‹œíŠ¸ ê¸°ë¡
    log_to_sheet(i+1, prob, label)

    if label == 1:
        print("\nğŸš¨ ê²½ê³  ë°œìƒ ì‹œì ì˜ ì›ë³¸ ë°ì´í„° (ìµœê·¼ 60ê°œ):")
        window_df = df.iloc[i:i+60][['Date'] + features]
        print(window_df.tail(60))
        log_window_data(window_df)

    time.sleep(1)  # 1ì´ˆ ê°„ê²© ìœ ì§€
